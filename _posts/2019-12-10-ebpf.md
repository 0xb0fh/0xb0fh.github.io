---
layout: post
title:  "Extended Berkeley Packet Filter 101"
date: 2019-12-10 20:06:26 +0100
categories: linux
---
Classic **BPF** were initially included in the kernel as a way for optimizing packet filtering.
For example, if you execute **tcpdump** with the **-d** option:

```sh
❯ sudo tcpdump -d port 80        
(000) ldh      [12]
(001) jeq      #0x86dd          jt 2    jf 10
(002) ldb      [20]
(003) jeq      #0x84            jt 6    jf 4
(004) jeq      #0x6             jt 6    jf 5
(005) jeq      #0x11            jt 6    jf 23
(006) ldh      [54]
(007) jeq      #0x50            jt 22   jf 8
(008) ldh      [56]
(009) jeq      #0x50            jt 22   jf 23
(010) jeq      #0x800           jt 11   jf 23
(011) ldb      [23]
(012) jeq      #0x84            jt 15   jf 13
(013) jeq      #0x6             jt 15   jf 14
(014) jeq      #0x11            jt 15   jf 23
(015) ldh      [20]
(016) jset     #0x1fff          jt 23   jf 17
(017) ldxb     4*([14]&0xf)
(018) ldh      [x + 14]
(019) jeq      #0x50            jt 22   jf 20
(020) ldh      [x + 16]
(021) jeq      #0x50            jt 22   jf 23
(022) ret      #262144
(023) ret      #0
```

you'll see the dump of the compiled code printed in a human readable form.
Using **-dd** instead will dump the code as C friendly, which is pretty convenient in case you need to use it with **setsockopt()** with the option **SO_ATTACH_FILTER** (see **man 7 socket**).

**eBPF** extends BPFs functionalities, giving the possibility to perform system tracing, allowing to execute code in kernel space but in a sandboxed environment.

**eBPFS** main limitations:
- no loops are allowed (unbounded loops basically)
- eBPF instructions number is limited

from ```man 2 bpf```:

```
eBPF programs can be written in a restricted C that is compiled (using the clang
compiler) into eBPF bytecode. Various features are omitted from this restricted
C, such as loops, global variables, variadic functions, floating-point numbers,
and passing structures as function arguments. Some examples can be found in the
samples/bpf/*_kern.c files in the kernel source tree.
```

Here ([{{site.source_baseurl}}/eBPF]({{site.source_baseurl}}/eBPF)) you can find a couple of examples of an eBPF program.

- ebpf_printk.c just writes to kernel's trace buffer file (/sys/kernel/debug/tracing/trace_pipe) if an ICMP packet pass through the eBPF filter.
- tcp_flags.c toggles the actual TCP flags (FIN, SYN, RST, PSH, ACK).

It worth to say that the kernel's trace buffer is shared among eBPF programs, so it's good for the examples above, in any other case it would be better to use BPF_PERF_OUTPUT().

To compile the examples you just need to:

```sh
> clang -O2 -target bpf -c tcp_flags.c -o bpf.o
```

which will generate:

```sh
❯ file tcp_flags.o
tcp_flags.o: ELF 64-bit LSB relocatable, eBPF, version 1 (SYSV), not stripped
```

which in turn, disassembling, has been translated to:

```sh
❯ llvm-objdump -no-show-raw-insn -section=action -S ./tcp_flags.o

./tcp_flags.o:  file format ELF64-BPF

Disassembly of section action:
0000000000000000 tcp_bpf_main:
       0:       r2 = *(u32 *)(r1 + 76)
       1:       r1 = *(u32 *)(r1 + 80)
       2:       r3 = r2
       3:       r3 += 54
       4:       if r3 > r1 goto +11 <LBB0_4>
       5:       r1 = *(u8 *)(r2 + 12)
       6:       r3 = *(u8 *)(r2 + 13)
       7:       r3 <<= 8
       8:       r3 |= r1
       9:       if r3 != 8 goto +6 <LBB0_4>
      10:       r1 = *(u8 *)(r2 + 23)
      11:       if r1 != 6 goto +4 <LBB0_4>
      12:       r2 += 34
      13:       r1 = *(u16 *)(r2 + 12)
      14:       r1 ^= 7936
      15:       *(u16 *)(r2 + 12) = r1

0000000000000080 LBB0_4:
      16:       r0 = 0
      17:       exit
```

To test tcp_flags.o you can create two namespaces:

```sh
> ip netns add ebpf-ns-client
> ip netns add ebpf-ns-server
```

then create and assign a veth pair to the corresponding namespaces:

```sh
> ip link add veth0 type veth peer name veth1
> ip link set veth0 netns ebpf-ns-server
> ip link set veth1 netns ebpf-ns-client
```

set the administrative status of the interface up, set the rules and then create a qdisc+egress/ingress filters for each namespace/veth like this (for ebpf-ns-client):

```sh
> ip netns exec ebpf-ns-client tc qdisc add dev veth1 clsact
> ip netns exec ebpf-ns-client tc filter add dev veth1 egress matchall action bpf object-file bpf.o sec action
> ip netns exec ebpf-ns-client tc filter add dev veth1 ingres matchall action bpf object-file bpf.o sec action
```

tc acts as a loader, attaching the eBPF program to the clsact hooks.

Then you can establish a TCP connection (e.g. using iperf3 or whatever tool) between ebpf-ns-{client,server}

From now on, if you start an iperf3 session from **ebpf-ns-client** to **ebpf-ns-server**, you'll see that the TCP packets traversing the two namespaces will have the flags the other way around they are supposed to be, but the packets are transferred without issues:

```sh
> ip netns exec ebpf-ns-server tcpdump -c 6 -tnni veth0 tcp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on veth0, link-type EN10MB (Ethernet), capture size 262144 bytes
IP 192.168.10.254.39220 > 192.168.10.1.5001: Flags [FRP.], seq 2830752969, ack 0, win 64240, options [mss 1460,sackOK,TS val 2612455419 ecr 0,nop,wscale 7], length 0
IP 192.168.10.1.5001 > 192.168.10.254.39220: Flags [FRP], seq 1132131708, win 65160, options [mss 1460,sackOK,TS val 3113876152 ecr 2612455419,nop,wscale 7], length 0
IP 192.168.10.254.39220 > 192.168.10.1.5001: Flags [FSRP], seq 2830752970, win 502, options [nop,nop,TS val 2612455419 ecr 3113876152], length 0
IP 192.168.10.254.39220 > 192.168.10.1.5001: Flags [FSRP], seq 2830752970:2830760210, win 502, options [nop,nop,TS val 2612455419 ecr 3113876152], length 7240
IP 192.168.10.1.5001 > 192.168.10.254.39220: Flags [FSRP], seq 1132131709, win 481, options [nop,nop,TS val 3113876152 ecr 2612455419], length 0
IP 192.168.10.254.39220 > 192.168.10.1.5001: Flags [FSRP], seq 2830760210:2830767450, win 502, options [nop,nop,TS val 2612455419 ecr 3113876152], length 7240
6 packets captured
11 packets received by filter
0 packets dropped by kernel
```

This is because the eBPF program attached to the ingress/egress clsact hooks replaces the flags way before the packets are processed by the linux TCP stack.

**<<EOF**
